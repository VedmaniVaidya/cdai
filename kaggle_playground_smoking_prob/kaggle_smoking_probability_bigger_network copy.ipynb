{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4BgGP4XVyCR7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# tok = userdata.get('git_token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WXHrGuIyIaA3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchmetrics -Uqq\n",
        "from torchmetrics.classification import BinaryAccuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "-F7nB-vR2lkS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3gknc2sE5jcY",
        "outputId": "ff8a8be3-1bfa-4899-ed4a-e359ff8efd7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "ictYw1g75riI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ClsModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, out_size, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "          nn.Linear(in_features=input_size, out_features=50, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=50, out_features=50, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=50, out_features=50, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(in_features=50, out_features=20, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.3),\n",
        "          nn.Linear(in_features=20, out_features=20, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=20, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=hidden_size, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=hidden_size, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=hidden_size, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=hidden_size, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.3),\n",
        "          nn.Linear(in_features=hidden_size, out_features=hidden_size, dtype=torch.float64),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(in_features=hidden_size, out_features=out_size, dtype=torch.float64),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    def forward(self, data):\n",
        "        return self.model(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# df = scaler.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "BEfxUKXL5wUp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_train(df):\n",
        "    categorical_cols = [\"hearing(left)\", \"hearing(right)\",\"dental caries\"]\n",
        "    numerical_cols = [col for col in df.columns if col not in categorical_cols]\n",
        "\n",
        "    df[\"bmi\"] = df[\"weight(kg)\"] / (df[\"height(cm)\"]/100)**2\n",
        "    df.drop([\"height(cm)\", \"weight(kg)\"], axis=1, inplace=True)\n",
        "\n",
        "    y = df[\"smoking\"]\n",
        "\n",
        "    numerical_cols.remove(\"height(cm)\")\n",
        "    numerical_cols.remove(\"weight(kg)\")\n",
        "    numerical_cols.remove(\"smoking\")\n",
        "    numerical_cols.append(\"bmi\")\n",
        "    df.drop([\"smoking\"],axis=1,inplace=True)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    return df.to_numpy(), y, scaler\n",
        "\n",
        "df=pd.read_csv(\"train.csv\", index_col='id')\n",
        "df,y,scl = prepare_train(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_test(df, scaler):\n",
        "    categorical_cols = [\"hearing(left)\", \"hearing(right)\", \"dental caries\"]\n",
        "    numerical_cols = [col for col in df.columns if col not in categorical_cols]\n",
        "    df[\"bmi\"] = df[\"weight(kg)\"] / (df[\"height(cm)\"]/100)**2\n",
        "    df.drop([\"height(cm)\", \"weight(kg)\"], axis=1, inplace=True)\n",
        "    numerical_cols.remove(\"height(cm)\")\n",
        "    numerical_cols.remove(\"weight(kg)\")\n",
        "    numerical_cols.append(\"bmi\")\n",
        "    df[numerical_cols] = scaler.transform(df[numerical_cols])\n",
        "    return df\n",
        "\n",
        "test = pd.read_csv(\"test.csv\", index_col='id')\n",
        "test = prepare_test(test, scl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_sample = self.X[idx]\n",
        "        y_sample = self.y.iloc[idx]\n",
        "        return x_sample.astype(\"float64\"), y_sample.astype(\"float64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "pMH6q7mp5ycJ"
      },
      "outputs": [],
      "source": [
        "# def get_data_loaders(X,y):\n",
        "#     X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,shuffle=True,stratify=y)\n",
        "#     train_dataset = CustomDataset(X,y)\n",
        "#     val_dataset = CustomDataset(X_test,y_test)\n",
        "#     train_loader = DataLoader(train_dataset,batch_size=32,num_workers=os.cpu_count(),pin_memory=True)\n",
        "#     val_loader = DataLoader(val_dataset,batch_size=32,num_workers=os.cpu_count(),pin_memory=True)\n",
        "#     return train_loader, val_loader\n",
        "# train_loader, val_loader = get_data_loaders(df,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Zwxlhg5650dY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def get_cv_data_loaders(X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, val_index in skf.split(X, y):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        train_dataset = CustomDataset(X_train, y_train)\n",
        "        val_dataset = CustomDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, num_workers=os.cpu_count(), pin_memory=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, num_workers=os.cpu_count(), pin_memory=True)\n",
        "\n",
        "        yield train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
        "    path = filename + '.pth'\n",
        "    torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "  }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    path = filename + '.pth'\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    model.eval()\n",
        "    return model, optimizer, epoch, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8ZyTRmUPA6ME"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model = ClsModel(22,10,1)\n",
        "model.to(device)\n",
        "# model.compile()\n",
        "loss_fn = nn.BCELoss()\n",
        "adam = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(adam, 'min', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQuAr1_G52X8",
        "outputId": "983612f2-6fa2-49ec-e5b5-71a4cdd85446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 0\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_10593/3620321131.py\", line 10, in __getitem__\n    x_sample = self.X[idx]\n               ~~~~~~^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network copy.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network%20copy.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m accumulated_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network%20copy.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network%20copy.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m idx,batch \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network%20copy.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     msg \u001b[39m=\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbatch: \u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dai/35/projects/kaggle_playground_smoking_prob/kaggle_smoking_probability_bigger_network%20copy.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m msg, end\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, flush\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 0\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_10593/3620321131.py\", line 10, in __getitem__\n    x_sample = self.X[idx]\n               ~~~~~~^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dai/anaconda3/envs/pytorch/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n"
          ]
        }
      ],
      "source": [
        "def training_loop():\n",
        "    for i in range(epochs):\n",
        "    #Training\n",
        "        model.train()\n",
        "        accumulated_loss = 0\n",
        "        print(f\"Epoch: {i}\")\n",
        "        for idx,batch in enumerate(train_loader):\n",
        "            msg = f\"batch: {idx}\"\n",
        "            print('\\r' + msg, end='', flush=True)\n",
        "            X,y = batch\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds,y.reshape(-1,1))\n",
        "            adam.zero_grad()\n",
        "            loss.backward()\n",
        "            adam.step()\n",
        "            accumulated_loss += loss.item()\n",
        "        print()\n",
        "        _loss = accumulated_loss/len(train_loader)\n",
        "        scheduler.step(_loss)\n",
        "        print(accumulated_loss/len(train_loader))\n",
        "        save_checkpoint(model, adam, i, accumulated_loss/len(train_loader), 'model')\n",
        "\n",
        "training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7mYUeuN6PdL"
      },
      "outputs": [],
      "source": [
        "# os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrEtq5lYC_qE"
      },
      "outputs": [],
      "source": [
        "# df=pd.read_csv(\"/content/train.csv\")\n",
        "# y=df[\"smoking\"]\n",
        "\n",
        "# df=df.drop([\"id\", \"smoking\"],axis=1)\n",
        "# X_train,X_test,y_train,y_test = train_test_split(df,y,random_state=42,shuffle=True,stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdY5XMiFDNcn"
      },
      "outputs": [],
      "source": [
        "# test_X = torch.Tensor(X_test.to_numpy()).to(torch.float64)\n",
        "# test_y = torch.Tensor(y_test.to_numpy()).to(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtslPeUnEPgd"
      },
      "outputs": [],
      "source": [
        "# metrics = BinaryAccuracy().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxIwHxsfCfcW"
      },
      "outputs": [],
      "source": [
        "# with torch.inference_mode():\n",
        "#     preds = model(test_X.to(device))\n",
        "#     acc = metrics(preds,test_y.reshape(-1,1).to(device))\n",
        "#     print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "p7GgdWRdKEdH"
      },
      "outputs": [],
      "source": [
        "# with torch.inference_mode():\n",
        "#     preds = model(test_X.to(device))\n",
        "#     acc = metrics(preds,test_y.reshape(-1,1).to(device))\n",
        "#     print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Jq4fTpfQKT9C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[5.6437e-01],\n",
            "        [3.6937e-01],\n",
            "        [1.4972e-01],\n",
            "        ...,\n",
            "        [4.2727e-01],\n",
            "        [6.2649e-02],\n",
            "        [1.3695e-05]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    preds = model(test.to(device))\n",
        "    print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DDr4RinTKbMo"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv(\"sample_submission.csv\", index_col=\"id\")\n",
        "submission['smoking'] = preds.cpu()\n",
        "submission.to_csv(\"submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
