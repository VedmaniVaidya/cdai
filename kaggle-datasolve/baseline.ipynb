{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('data/distilbert-base-uncased_tokenized_dataset')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['labels', 'input_ids', 'attention_mask'],\n",
       " 'test': ['labels', 'input_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data collator\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#create data loader\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "         1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'input_ids': tensor([[  101,  1041,  3676,  ...,     0,     0,     0],\n",
      "        [  101,  6622,  1997,  ...,     0,     0,     0],\n",
      "        [  101,  3795,  1060,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2019,  2552,  ...,     0,     0,     0],\n",
      "        [  101,  3010, 12012,  ...,     0,     0,     0],\n",
      "        [  101,  4861,  2012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1041,  3676,  ...,     0,     0,     0],\n",
       "        [  101,  6622,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  3795,  1060,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2019,  2552,  ...,     0,     0,     0],\n",
       "        [  101,  3010, 12012,  ...,     0,     0,     0],\n",
       "        [  101,  4861,  2012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76aa0f1bf6a242fa8b2eee1b26f29bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=50, problem_type='multi_label_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0617,  0.0664, -0.0906,  0.1200, -0.1792,  0.0214, -0.0105,  0.1984,\n",
      "          0.1139, -0.1736,  0.0925,  0.0239, -0.1112, -0.0003,  0.0889, -0.1375,\n",
      "         -0.0091,  0.0537,  0.0994,  0.0878,  0.1000,  0.0403, -0.0995,  0.0165,\n",
      "         -0.1018,  0.1070, -0.0768, -0.1758,  0.2154, -0.0615, -0.1563,  0.1224,\n",
      "         -0.0645,  0.0374, -0.0796, -0.0093,  0.0024,  0.0665,  0.1369,  0.0858,\n",
      "         -0.0881,  0.0556,  0.0411,  0.0982, -0.0972, -0.0228,  0.0071,  0.0676,\n",
      "         -0.1466, -0.1093],\n",
      "        [ 0.0366,  0.0838, -0.1000,  0.0857, -0.1110,  0.0560, -0.0626,  0.1722,\n",
      "          0.1241, -0.1956,  0.0987,  0.0126, -0.1091, -0.0401,  0.1115, -0.1211,\n",
      "         -0.0608,  0.0620,  0.0470,  0.0753,  0.0720, -0.0136, -0.0844, -0.0160,\n",
      "         -0.0615,  0.1050, -0.0488, -0.1687,  0.1910, -0.0127, -0.1063,  0.1125,\n",
      "         -0.0453,  0.0122, -0.0347, -0.0080,  0.0497,  0.0742,  0.0893,  0.1397,\n",
      "         -0.1149,  0.0124,  0.0150,  0.0790, -0.0825,  0.0229, -0.0100,  0.0425,\n",
      "         -0.1061, -0.1003],\n",
      "        [-0.0083,  0.0167, -0.1410,  0.0796, -0.1041,  0.0654, -0.0076,  0.2137,\n",
      "          0.1694, -0.2485,  0.0475,  0.0575, -0.1334, -0.0525,  0.0624, -0.0823,\n",
      "         -0.0279, -0.0012,  0.0856,  0.0676,  0.1556, -0.0007, -0.0865,  0.0419,\n",
      "         -0.1056,  0.1040, -0.0852, -0.2332,  0.1741, -0.0258, -0.1681,  0.1510,\n",
      "          0.0055,  0.0635, -0.0835, -0.0187,  0.0183,  0.0683,  0.1568,  0.1074,\n",
      "         -0.1322, -0.0148,  0.0090,  0.1105, -0.1080, -0.0010, -0.0700,  0.1015,\n",
      "         -0.1472, -0.0452],\n",
      "        [ 0.0313,  0.0531, -0.0877,  0.0902, -0.1318,  0.0609, -0.0663,  0.2642,\n",
      "          0.0726, -0.2126,  0.0895, -0.0187, -0.1256, -0.0050,  0.1222, -0.1580,\n",
      "         -0.0275,  0.0843,  0.0477,  0.0566,  0.0504, -0.0158, -0.1067, -0.0222,\n",
      "         -0.0817,  0.0793, -0.0692, -0.0947,  0.2483, -0.0252, -0.1838,  0.1008,\n",
      "         -0.0223,  0.0205, -0.0508, -0.0278,  0.0449,  0.0644,  0.1289,  0.1268,\n",
      "         -0.0773,  0.0329,  0.0813,  0.1286, -0.1031, -0.0129, -0.0239,  0.0158,\n",
      "         -0.1205, -0.2025],\n",
      "        [-0.0119, -0.0354, -0.1538,  0.0883, -0.1512,  0.0099, -0.0062,  0.1866,\n",
      "          0.1452, -0.2219,  0.0794,  0.0749, -0.0620, -0.0495,  0.1112, -0.1663,\n",
      "          0.0031,  0.0659,  0.0766,  0.1069,  0.0786,  0.0240, -0.0713,  0.0552,\n",
      "         -0.1392,  0.0708, -0.0735, -0.1837,  0.1901, -0.0153, -0.1522,  0.1241,\n",
      "          0.0006,  0.0219, -0.0004,  0.0015, -0.0190,  0.0692,  0.1540,  0.0973,\n",
      "         -0.0690,  0.0345,  0.0229,  0.0645, -0.0707,  0.0116, -0.0308,  0.0504,\n",
      "         -0.0896, -0.1105],\n",
      "        [ 0.0066,  0.0553, -0.1053,  0.0781, -0.1845,  0.0655,  0.0331,  0.2041,\n",
      "          0.1202, -0.1868,  0.0595,  0.0166, -0.1344,  0.0017,  0.0845, -0.1440,\n",
      "          0.0167,  0.0210,  0.0284,  0.0556,  0.1019,  0.0326, -0.0843, -0.0126,\n",
      "         -0.0316,  0.0822, -0.1136, -0.2138,  0.1202, -0.0321, -0.1585,  0.1295,\n",
      "         -0.0791,  0.0457, -0.0711, -0.0249,  0.0048,  0.0684,  0.1058,  0.1132,\n",
      "         -0.0721,  0.0208, -0.0234,  0.1048, -0.0343,  0.0364, -0.0298,  0.0079,\n",
      "         -0.1286, -0.1207],\n",
      "        [-0.0322,  0.0197, -0.1540,  0.0887, -0.1156,  0.0333,  0.0196,  0.2068,\n",
      "          0.1462, -0.2140,  0.1036,  0.0429, -0.0801, -0.0252,  0.0509, -0.0859,\n",
      "         -0.0078,  0.0265,  0.0517,  0.0774,  0.0992,  0.0401, -0.0699,  0.0386,\n",
      "         -0.0896,  0.0822, -0.0951, -0.1895,  0.1552, -0.0193, -0.1616,  0.1413,\n",
      "         -0.0172,  0.0123, -0.0558,  0.0135,  0.0097,  0.0682,  0.1175,  0.1692,\n",
      "         -0.0871, -0.0048, -0.0207,  0.0911, -0.0896,  0.0478, -0.0826,  0.0466,\n",
      "         -0.1376, -0.0870],\n",
      "        [-0.0298,  0.0391, -0.1284,  0.0788, -0.2178,  0.1186,  0.0325,  0.1885,\n",
      "          0.2152, -0.2330,  0.0917, -0.0049, -0.1409, -0.0491,  0.1648, -0.1455,\n",
      "         -0.0740,  0.1412,  0.0365,  0.1320,  0.0692,  0.0284, -0.0630, -0.0278,\n",
      "         -0.1323,  0.1006, -0.1527, -0.1501,  0.1700, -0.0330, -0.1612,  0.1383,\n",
      "         -0.0634,  0.0196,  0.0262, -0.0609,  0.0267,  0.0056,  0.0990,  0.1542,\n",
      "         -0.0875,  0.0137,  0.0138,  0.1060, -0.0889, -0.0182, -0.0344,  0.0642,\n",
      "         -0.1705, -0.1422],\n",
      "        [-0.0332,  0.0327, -0.0924,  0.0704, -0.1092, -0.0047, -0.0099,  0.1265,\n",
      "          0.1630, -0.1881,  0.0339,  0.0204, -0.1105, -0.0155,  0.0672, -0.0858,\n",
      "          0.0134,  0.0156,  0.0808,  0.1139,  0.0595,  0.0767, -0.0954,  0.0601,\n",
      "         -0.1208,  0.0860, -0.0572, -0.2119,  0.1493,  0.0078, -0.1454,  0.0916,\n",
      "         -0.0079,  0.0295, -0.0426,  0.0370, -0.0153,  0.0696,  0.1265,  0.1151,\n",
      "         -0.0971, -0.0178, -0.0286,  0.0595, -0.0856, -0.0029, -0.0484,  0.0996,\n",
      "         -0.1277, -0.0546],\n",
      "        [ 0.0130, -0.0558, -0.1963,  0.1010, -0.1377,  0.0742,  0.0046,  0.1966,\n",
      "          0.1407, -0.2026,  0.0446,  0.0743, -0.0459, -0.0357,  0.0848, -0.1121,\n",
      "         -0.0195,  0.0736,  0.0576,  0.0998,  0.0585,  0.0587, -0.0684,  0.0007,\n",
      "         -0.1265,  0.0481, -0.1059, -0.1937,  0.2352, -0.0811, -0.1743,  0.1593,\n",
      "          0.0082,  0.0071, -0.0436,  0.0136, -0.0583,  0.0999,  0.1405,  0.0824,\n",
      "         -0.0815,  0.0047,  0.0384,  0.0549, -0.0895,  0.0186, -0.0596,  0.0559,\n",
      "         -0.1187, -0.1001],\n",
      "        [ 0.0361,  0.0097, -0.1830,  0.0913, -0.2046, -0.0055,  0.0430,  0.2054,\n",
      "          0.0911, -0.1924,  0.0794, -0.0042, -0.1034, -0.0395,  0.1053, -0.1530,\n",
      "         -0.0208,  0.1208,  0.0821,  0.1136,  0.1080,  0.0031, -0.0842,  0.0334,\n",
      "         -0.1010,  0.0649, -0.0384, -0.2188,  0.2010, -0.0225, -0.1656,  0.1111,\n",
      "         -0.0438,  0.0045, -0.0453,  0.0054, -0.0037,  0.0606,  0.1540,  0.0964,\n",
      "         -0.0803,  0.0441,  0.0763,  0.0528, -0.0895,  0.0070, -0.0253,  0.0733,\n",
      "         -0.0960, -0.1553],\n",
      "        [-0.0359, -0.0301, -0.1434,  0.0827, -0.1410,  0.0024,  0.0082,  0.1930,\n",
      "          0.1730, -0.2363,  0.0771,  0.0316, -0.1142, -0.0482,  0.1002, -0.1406,\n",
      "         -0.0045,  0.0561,  0.0888,  0.0901,  0.1213,  0.0463, -0.1094,  0.0626,\n",
      "         -0.1336,  0.0845, -0.0650, -0.1663,  0.1889, -0.0673, -0.1762,  0.1398,\n",
      "         -0.0210,  0.0176, -0.0507, -0.0366, -0.0282,  0.0923,  0.1935,  0.0802,\n",
      "         -0.0916,  0.0197,  0.0280,  0.0880, -0.0794, -0.0144, -0.0637,  0.0915,\n",
      "         -0.1333, -0.0802],\n",
      "        [ 0.0314,  0.0024, -0.1164,  0.1050, -0.1174,  0.0433, -0.0241,  0.2027,\n",
      "          0.0855, -0.1881,  0.0611, -0.0160, -0.1370, -0.0163,  0.1029, -0.1096,\n",
      "          0.0406,  0.0674,  0.0538,  0.0866,  0.0576,  0.0212, -0.0864,  0.0421,\n",
      "         -0.1122,  0.0769, -0.1030, -0.1841,  0.2325, -0.0220, -0.1622,  0.1511,\n",
      "         -0.0697,  0.0128, -0.0755, -0.0222,  0.0033,  0.0819,  0.1520,  0.0964,\n",
      "         -0.0792,  0.0549,  0.0264,  0.0807, -0.0675, -0.0276, -0.0011,  0.0552,\n",
      "         -0.1409, -0.1076],\n",
      "        [-0.0419, -0.0061, -0.1382,  0.0810, -0.1268,  0.0231,  0.0837,  0.2095,\n",
      "          0.1218, -0.2375,  0.0127,  0.0484, -0.1188, -0.0211,  0.1505, -0.0829,\n",
      "          0.0078,  0.1013,  0.0933,  0.1188,  0.1074,  0.0937, -0.0527,  0.0065,\n",
      "         -0.1485,  0.0643, -0.0581, -0.1631,  0.1219, -0.0269, -0.2277,  0.1151,\n",
      "         -0.0318,  0.0044, -0.0153, -0.0046, -0.0412,  0.0412,  0.1484,  0.1084,\n",
      "         -0.0241, -0.0085,  0.0246,  0.1076,  0.0071,  0.0209, -0.0717,  0.0634,\n",
      "         -0.0853, -0.0783],\n",
      "        [-0.0418,  0.0108, -0.1696,  0.0767, -0.0931,  0.0355, -0.0519,  0.2130,\n",
      "          0.1778, -0.2196,  0.0371,  0.0315, -0.1191, -0.0762,  0.0901, -0.1039,\n",
      "         -0.0183,  0.0060,  0.0856,  0.1165,  0.1206,  0.0040, -0.0776,  0.0399,\n",
      "         -0.1405,  0.1085, -0.0742, -0.1973,  0.2147, -0.0501, -0.1506,  0.0945,\n",
      "         -0.0571,  0.0409, -0.0290, -0.0103, -0.0101,  0.0724,  0.1892,  0.1195,\n",
      "         -0.1247,  0.0024,  0.0091,  0.1044, -0.0925, -0.0231, -0.0303,  0.0822,\n",
      "         -0.0777, -0.0440],\n",
      "        [-0.0051,  0.0316, -0.0946,  0.1137, -0.1238,  0.0765, -0.0266,  0.2498,\n",
      "          0.1333, -0.1651,  0.1223,  0.0055, -0.1480, -0.0481,  0.0887, -0.1455,\n",
      "          0.0340,  0.0268,  0.0320,  0.0473,  0.1335, -0.0033, -0.0488,  0.0333,\n",
      "         -0.0920,  0.1052, -0.0799, -0.1460,  0.2052, -0.0513, -0.1793,  0.1401,\n",
      "         -0.0697,  0.0101, -0.0645, -0.0467,  0.0256,  0.0623,  0.1165,  0.0995,\n",
      "         -0.0773,  0.0526,  0.0388,  0.0923, -0.0875, -0.0060, -0.0202,  0.0131,\n",
      "         -0.1649, -0.1219]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
