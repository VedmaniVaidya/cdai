{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 08:20:27.075965: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 08:20:27.517903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 7887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('data/distilbert-base-uncased_tokenized_dataset')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['labels', 'input_ids', 'attention_mask'],\n",
       " 'test': ['labels', 'input_ids', 'attention_mask']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data collator\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#create data loader\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "         1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'input_ids': tensor([[  101,  1041,  3676,  ...,     0,     0,     0],\n",
      "        [  101,  6622,  1997,  ...,     0,     0,     0],\n",
      "        [  101,  3795,  1060,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2019,  2552,  ...,     0,     0,     0],\n",
      "        [  101,  3010, 12012,  ...,     0,     0,     0],\n",
      "        [  101,  4861,  2012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1041,  3676,  ...,     0,     0,     0],\n",
       "        [  101,  6622,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  3795,  1060,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2019,  2552,  ...,     0,     0,     0],\n",
       "        [  101,  3010, 12012,  ...,     0,     0,     0],\n",
       "        [  101,  4861,  2012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=50, problem_type='multi_label_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-5.3810e-02, -6.5098e-02,  4.5533e-02, -2.7215e-02,  2.6312e-02,\n",
      "          4.3021e-03,  1.1469e-01, -7.3923e-05,  1.0586e-01, -9.3026e-02,\n",
      "         -3.2912e-02,  4.3366e-02, -3.0034e-02,  1.4252e-01, -1.4582e-01,\n",
      "          1.7466e-01,  6.4193e-02,  1.5631e-01,  9.6937e-02,  6.6014e-02,\n",
      "          3.7520e-02,  4.4540e-02,  1.2697e-01, -4.9129e-02, -1.9707e-01,\n",
      "          5.7044e-02, -3.9536e-02,  6.9872e-02,  5.9503e-02,  6.7755e-02,\n",
      "          5.4143e-02,  7.1272e-02,  8.7636e-02, -1.1233e-01, -1.5267e-01,\n",
      "         -3.9699e-02,  9.4313e-02,  7.0356e-02,  5.7860e-02, -8.4137e-02,\n",
      "          7.9250e-02, -4.7304e-02, -9.6850e-02,  3.7648e-02, -1.0777e-02,\n",
      "         -3.1993e-02,  5.4081e-02, -6.4268e-02, -2.4575e-03,  2.0928e-03],\n",
      "        [-6.7155e-02, -1.0279e-02,  2.6296e-02, -2.3047e-02, -7.4425e-03,\n",
      "         -2.6223e-03,  9.4427e-02, -1.0079e-02,  6.4856e-02, -4.0869e-02,\n",
      "          8.6453e-03,  3.1303e-02,  1.2334e-03,  1.2009e-01, -1.4612e-01,\n",
      "          2.0658e-01,  2.8499e-02,  1.0419e-01,  1.6009e-01,  2.2255e-02,\n",
      "          4.2356e-02,  5.9254e-02,  6.2181e-02, -6.8274e-02, -1.5843e-01,\n",
      "          4.1133e-02, -4.4042e-02,  6.5739e-02,  5.0194e-02,  4.0563e-02,\n",
      "          6.7332e-02,  2.5945e-02,  8.5908e-02, -1.0533e-01, -1.6549e-01,\n",
      "         -1.1656e-02,  1.5090e-01,  3.6864e-02,  5.0778e-02, -9.0014e-02,\n",
      "          2.8246e-02, -5.2852e-02, -8.0112e-02,  2.9957e-02,  1.9686e-02,\n",
      "          2.1360e-02,  1.7183e-02, -5.9512e-02,  4.6996e-02, -5.1215e-02],\n",
      "        [-1.0571e-01, -8.6198e-02,  2.4858e-02, -5.0809e-02,  3.8667e-02,\n",
      "          2.4711e-02,  1.2232e-01, -5.3945e-02,  5.7568e-02, -3.8490e-02,\n",
      "         -6.0053e-02,  9.4670e-02, -1.6342e-02,  1.3633e-01, -9.7791e-02,\n",
      "          1.9236e-01,  8.9754e-02,  1.5848e-01,  8.2992e-02,  4.4585e-02,\n",
      "          6.6175e-02,  8.2794e-02,  1.1727e-01, -5.9329e-02, -1.8680e-01,\n",
      "          8.2441e-02,  1.6219e-02,  1.1470e-01,  3.5262e-02,  1.1799e-01,\n",
      "          1.0477e-01,  6.3150e-03,  3.2503e-02, -1.7520e-01, -2.0753e-01,\n",
      "         -2.6215e-02,  1.4726e-01,  2.3311e-02,  6.7605e-02, -9.1716e-02,\n",
      "         -6.2962e-03, -4.8935e-02, -1.6891e-01,  7.3357e-02,  5.7249e-02,\n",
      "          2.5699e-02,  2.4256e-02, -2.4839e-02, -5.4924e-02, -3.5726e-02],\n",
      "        [-9.6297e-02, -7.1254e-02,  8.5713e-03,  3.2742e-02,  2.2085e-02,\n",
      "         -2.8893e-02,  1.3021e-01, -4.1879e-02,  8.6471e-02, -6.7259e-02,\n",
      "         -2.7879e-02,  7.1055e-02, -2.1004e-02,  1.6218e-01, -1.3684e-01,\n",
      "          1.6853e-01,  4.6195e-02,  1.2451e-01,  1.6113e-01,  1.0369e-01,\n",
      "          9.8255e-02,  7.0000e-02,  9.4062e-02, -3.6116e-02, -2.1459e-01,\n",
      "          5.7116e-02, -5.5250e-02,  1.2667e-01,  4.2857e-02,  5.3015e-02,\n",
      "          4.5968e-02,  4.9904e-02,  8.4269e-02, -1.4767e-01, -1.7149e-01,\n",
      "          1.0253e-02,  6.9811e-02,  5.7622e-03,  9.3353e-02, -9.6152e-02,\n",
      "          6.1042e-02, -3.1375e-02, -1.3140e-01,  8.2423e-02,  2.5634e-02,\n",
      "          1.9708e-02,  5.5459e-02,  6.1799e-03,  1.5385e-02, -5.6890e-02],\n",
      "        [-9.0609e-02, -9.4952e-02,  3.1728e-02, -4.4981e-02,  3.8529e-02,\n",
      "          3.8120e-02,  1.2205e-01,  1.9867e-03,  1.0500e-01, -9.7643e-02,\n",
      "          1.6544e-02,  4.0143e-02, -1.9068e-02,  6.1264e-02, -1.1300e-01,\n",
      "          1.7330e-01,  4.3551e-03,  1.0905e-01,  1.1597e-01,  4.1093e-02,\n",
      "          4.5630e-02,  3.1415e-02,  1.1424e-01, -4.4911e-02, -1.9613e-01,\n",
      "          8.1206e-02, -2.4046e-02,  6.5402e-02,  4.0794e-02,  7.5057e-02,\n",
      "          5.3345e-02,  2.1270e-02,  7.3891e-02, -1.5533e-01, -2.0194e-01,\n",
      "         -3.5569e-02,  1.0207e-01, -2.2780e-02,  6.7001e-02, -1.0784e-01,\n",
      "          1.6629e-02, -6.9475e-03, -1.2036e-01,  5.5008e-02,  7.7330e-02,\n",
      "          1.9021e-02,  6.7496e-03, -5.2520e-02, -3.9308e-02, -2.6331e-02],\n",
      "        [-1.1140e-01, -7.8022e-02, -1.4988e-03, -6.3951e-02,  4.3971e-02,\n",
      "         -2.3114e-02,  9.4442e-02, -2.2867e-02,  8.1764e-02, -1.1112e-01,\n",
      "          3.5849e-02,  4.2024e-02, -1.1521e-02,  1.3733e-01, -1.0917e-01,\n",
      "          1.5046e-01,  8.2864e-02,  1.4460e-01,  9.6544e-02,  8.3090e-02,\n",
      "          1.0475e-01,  1.6750e-02,  1.2661e-01, -6.2911e-02, -1.8079e-01,\n",
      "          8.4307e-02, -8.1541e-03,  9.9716e-02,  4.6163e-02,  7.1514e-02,\n",
      "          5.2454e-02,  6.7991e-02,  5.1820e-02, -1.2605e-01, -1.4462e-01,\n",
      "         -2.3909e-02,  8.7052e-02,  7.5786e-02,  6.7297e-03, -9.2538e-02,\n",
      "         -1.5907e-03, -8.1461e-02, -6.4078e-02,  6.1042e-02, -3.6830e-03,\n",
      "          1.6846e-02,  4.4220e-02, -1.5795e-02, -1.8532e-02, -4.0557e-02],\n",
      "        [-1.2139e-01, -6.8775e-02, -1.4462e-02, -6.2659e-02,  5.6837e-02,\n",
      "          3.0024e-03,  1.1736e-01,  2.2726e-02,  1.0424e-01, -7.9583e-02,\n",
      "         -1.6306e-03,  9.9030e-02, -9.1805e-03,  1.2068e-01, -1.1285e-01,\n",
      "          2.0163e-01,  6.5136e-02,  1.5091e-01,  4.5112e-02,  6.7185e-02,\n",
      "          9.4950e-02,  4.8420e-02,  1.2831e-01, -3.7911e-02, -1.7832e-01,\n",
      "          9.9407e-02, -1.3020e-04,  1.3222e-01,  2.4232e-02,  8.5364e-02,\n",
      "          6.7833e-02, -2.2264e-02,  4.0948e-02, -1.8777e-01, -1.8247e-01,\n",
      "         -1.3162e-02,  1.3088e-01,  2.0850e-02,  8.2512e-02, -1.0533e-01,\n",
      "          5.9045e-03, -3.6935e-02, -1.0115e-01,  5.5206e-02,  5.6948e-02,\n",
      "          2.5133e-02,  1.6805e-02,  7.8218e-03, -5.1084e-02, -1.2254e-02],\n",
      "        [-9.8301e-02, -1.1570e-01,  2.0928e-02, -2.2187e-02, -2.1707e-02,\n",
      "         -1.0296e-02,  1.6479e-01,  1.3699e-02,  6.1008e-02, -1.0396e-01,\n",
      "         -4.4632e-02,  9.8099e-02,  1.2637e-02,  7.4914e-02, -7.5716e-02,\n",
      "          1.1984e-01,  7.5045e-02,  1.0367e-01,  1.0556e-01,  9.6840e-02,\n",
      "          7.7833e-02,  4.4534e-02,  6.3630e-02, -3.6174e-02, -2.0566e-01,\n",
      "          7.9593e-02, -2.6677e-02,  1.0412e-01,  1.5089e-01,  4.2271e-02,\n",
      "          4.0218e-02,  6.3430e-02,  1.1637e-01, -9.9564e-02, -1.8767e-01,\n",
      "         -1.4963e-02,  1.7580e-01, -4.9046e-03,  5.9739e-02, -1.0018e-01,\n",
      "          7.1094e-02, -9.6856e-02, -1.0974e-01,  1.2845e-02,  4.5646e-02,\n",
      "         -1.7206e-02,  4.0892e-02, -9.1317e-02, -1.5303e-02, -6.2843e-02],\n",
      "        [-7.5476e-02, -1.8182e-02, -2.9530e-03, -4.1701e-02,  4.8221e-02,\n",
      "         -9.0887e-03,  9.0034e-02, -1.2508e-02,  3.3317e-02, -6.9939e-02,\n",
      "         -9.8213e-04,  6.3516e-02,  2.1625e-02,  1.0756e-01, -9.5428e-02,\n",
      "          1.5527e-01,  2.8059e-02,  1.3266e-01,  5.6556e-02,  1.6390e-02,\n",
      "          5.0392e-02,  4.0411e-02,  1.1004e-01, -4.2979e-02, -1.9056e-01,\n",
      "          1.2911e-01,  3.2345e-02,  5.7429e-02,  4.8844e-03,  8.8101e-02,\n",
      "          1.1932e-01,  4.0748e-02,  1.7194e-02, -1.3182e-01, -1.2460e-01,\n",
      "         -3.8239e-02,  9.7308e-02,  2.0996e-02,  8.1406e-02, -7.1473e-02,\n",
      "          6.5851e-03, -4.0044e-02, -7.7743e-02,  3.2555e-02,  2.7277e-02,\n",
      "         -2.8463e-02,  3.6394e-02, -7.1404e-02, -2.3960e-02, -1.1969e-02],\n",
      "        [-6.2989e-02, -9.2893e-02,  1.5382e-03, -3.1114e-02,  6.7152e-02,\n",
      "          9.4537e-02,  1.0140e-01,  3.9621e-02,  1.4212e-01, -4.7552e-02,\n",
      "         -5.6955e-02,  4.1540e-02, -7.1502e-02,  9.5558e-02, -1.4634e-01,\n",
      "          2.0978e-01,  1.3574e-02,  9.0723e-02,  5.3400e-02,  7.9899e-02,\n",
      "          6.4140e-02,  3.8639e-02,  1.1052e-01, -5.6700e-02, -1.5595e-01,\n",
      "          5.7278e-02, -3.3279e-03,  9.1264e-02,  3.7239e-02,  5.7291e-02,\n",
      "          5.7389e-02, -2.1851e-03,  6.1470e-02, -1.3022e-01, -1.5385e-01,\n",
      "         -3.0631e-02,  1.1073e-01, -4.0737e-02,  8.5332e-03, -7.0940e-02,\n",
      "          2.4953e-02, -6.3188e-02, -9.4802e-02,  3.9928e-02,  7.0038e-02,\n",
      "          4.4918e-02,  2.9388e-03, -6.0761e-03, -7.6766e-02,  1.3637e-02],\n",
      "        [-1.0742e-01, -6.5727e-02, -1.1774e-02, -2.9201e-02,  4.5785e-02,\n",
      "          1.6689e-02,  1.5072e-01,  4.0643e-02,  8.6520e-02, -6.4322e-02,\n",
      "         -1.9176e-02,  5.5531e-02, -5.2199e-03,  7.9323e-02, -1.5922e-01,\n",
      "          1.3225e-01,  1.4101e-02,  1.0373e-01,  1.5525e-01,  2.4736e-02,\n",
      "          4.1414e-02,  4.0883e-02,  1.5800e-01, -4.4777e-02, -2.1662e-01,\n",
      "          7.9380e-02, -9.6296e-02,  1.1987e-01,  2.8409e-02,  8.4597e-02,\n",
      "          3.0071e-02,  1.9369e-03,  5.0753e-02, -1.5404e-01, -2.3450e-01,\n",
      "         -8.9397e-02,  1.0982e-01, -1.9321e-02,  8.6339e-02, -1.5039e-01,\n",
      "          3.4366e-02, -7.7981e-02, -1.1795e-01,  3.9772e-02,  1.6659e-02,\n",
      "         -5.3639e-03,  3.3313e-02, -2.9615e-02, -1.5963e-03, -4.0625e-02],\n",
      "        [-8.3546e-02, -4.2782e-02,  2.8394e-02, -8.6948e-02,  5.5444e-02,\n",
      "          1.9351e-02,  1.2797e-01, -3.7465e-02,  8.5481e-02, -8.5875e-02,\n",
      "         -5.6211e-02,  6.1548e-02, -1.7240e-02,  1.3825e-01, -1.3837e-01,\n",
      "          1.7272e-01,  3.9460e-02,  1.3528e-01,  8.1189e-02,  6.0223e-02,\n",
      "          6.7964e-02,  8.4732e-02,  1.0995e-01, -2.4936e-02, -1.7346e-01,\n",
      "          1.0164e-01, -1.6026e-02,  8.9011e-02,  5.3998e-02,  7.0777e-02,\n",
      "          1.0061e-01, -3.5954e-02,  6.0297e-02, -1.6161e-01, -1.6044e-01,\n",
      "         -5.1421e-02,  1.0191e-01,  1.3093e-02,  1.0465e-01, -9.4298e-02,\n",
      "          4.5732e-02, -1.1961e-02, -1.7333e-01,  6.2750e-02,  2.7927e-02,\n",
      "          1.9821e-02, -3.2349e-02, -4.8758e-02, -3.2867e-02, -9.2345e-03],\n",
      "        [-1.1192e-01, -3.4666e-02,  5.3958e-03, -5.4293e-02,  7.0441e-02,\n",
      "          4.0774e-02,  1.0236e-01, -4.7286e-02,  1.4515e-01, -1.4550e-01,\n",
      "          2.4551e-02,  8.2399e-02, -2.7939e-02,  1.0959e-01, -1.0266e-01,\n",
      "          1.4570e-01,  5.8852e-03,  1.6552e-01,  1.0601e-01,  4.9542e-02,\n",
      "          4.4277e-02,  7.1896e-02,  1.0137e-01, -1.4872e-02, -1.9189e-01,\n",
      "          1.1440e-01, -2.0803e-02,  1.0120e-01,  4.2080e-02,  7.0344e-02,\n",
      "          3.8862e-02,  3.9751e-02,  9.7394e-02, -1.3026e-01, -1.2387e-01,\n",
      "         -3.3527e-02,  1.4921e-01,  3.6951e-02,  9.3906e-02, -1.2047e-01,\n",
      "          1.5143e-02, -5.3845e-02, -1.2320e-01,  8.8817e-02,  2.1691e-02,\n",
      "         -3.2428e-02,  4.4790e-02, -6.0301e-02, -1.9900e-02, -3.5704e-02],\n",
      "        [-8.8225e-02, -1.7074e-02,  5.8988e-02, -6.0307e-02,  7.2565e-02,\n",
      "         -2.6851e-02,  6.1836e-02,  1.8209e-03,  1.0564e-01, -1.0585e-01,\n",
      "         -2.5115e-04,  2.8546e-02, -1.4354e-02,  6.0348e-02, -1.0552e-01,\n",
      "          1.7444e-01,  3.4684e-02,  9.3187e-02,  5.8302e-02,  4.7149e-02,\n",
      "          1.0080e-01,  4.9396e-02,  1.2123e-01, -2.7545e-02, -2.0083e-01,\n",
      "          1.3146e-01, -5.4477e-02,  9.0545e-02,  5.5229e-02,  5.7444e-02,\n",
      "          5.2470e-02,  4.9842e-02, -1.2801e-02, -1.3601e-01, -1.2940e-01,\n",
      "         -5.0302e-02,  1.2528e-01, -1.1780e-02,  6.7323e-02, -1.3825e-01,\n",
      "          3.7754e-02, -7.9969e-02, -5.1919e-02,  1.2753e-01,  3.9350e-02,\n",
      "          2.7433e-02,  3.6314e-02, -6.5718e-02,  3.4733e-02, -4.5910e-02],\n",
      "        [-4.0289e-02, -4.7477e-02,  4.6732e-02, -1.3562e-02,  3.8231e-02,\n",
      "          5.5247e-02,  1.0840e-01, -2.6703e-02,  4.5369e-02, -5.6928e-02,\n",
      "         -2.8603e-02,  4.6538e-02, -6.1212e-03,  1.2771e-01, -8.3276e-02,\n",
      "          1.7284e-01,  4.9591e-02,  1.5120e-01,  5.7148e-02,  3.9059e-02,\n",
      "          6.8475e-02,  5.4221e-02,  1.0625e-01, -5.2508e-02, -2.0038e-01,\n",
      "          1.0775e-01, -1.4936e-02,  1.1899e-01,  6.3655e-02,  9.1765e-02,\n",
      "          8.9422e-02,  9.7636e-03,  2.0619e-02, -1.8539e-01, -1.5063e-01,\n",
      "         -2.9963e-02,  1.3795e-01, -7.9885e-03,  4.1885e-02, -1.2245e-01,\n",
      "          6.0764e-03, -3.9028e-02, -1.3110e-01,  6.9283e-02,  3.5130e-02,\n",
      "         -3.8441e-02,  3.4654e-02, -4.9968e-02, -3.5397e-02, -1.0501e-02],\n",
      "        [-1.1380e-01, -1.0937e-01,  1.6598e-03, -4.4445e-02,  5.9021e-02,\n",
      "         -7.5413e-03,  5.6711e-02, -2.5669e-02,  7.9213e-02, -8.5798e-02,\n",
      "         -1.6659e-02,  9.7919e-02,  3.9059e-02,  1.0592e-01, -1.3090e-01,\n",
      "          1.6174e-01,  6.7921e-02,  1.9538e-01,  1.1592e-01,  3.3036e-02,\n",
      "          7.8150e-02,  1.0763e-01,  7.4846e-02, -2.2497e-02, -1.6381e-01,\n",
      "          7.9257e-02, -6.8145e-02,  1.2102e-01,  5.1730e-02,  6.8067e-02,\n",
      "          2.6557e-02,  7.9924e-03,  1.2595e-01, -1.2593e-01, -1.8250e-01,\n",
      "         -1.8989e-04,  1.5757e-01,  7.6073e-02,  2.6469e-02, -1.1542e-01,\n",
      "         -1.7653e-02, -1.1632e-01, -1.0320e-01,  3.6179e-02,  4.8502e-02,\n",
      "          1.6805e-02,  3.4062e-02, -4.4545e-02, -4.7058e-02, -6.9915e-02]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**batch)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6998)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate loss for multi-label classification\n",
    "import torch.nn as nn\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "loss(output.logits, labels.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
